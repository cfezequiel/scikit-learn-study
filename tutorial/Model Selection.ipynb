{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score, and cross-validated scores \n",
    "\n",
    "As we have seen, every estimator exposes a score method that can judge the quality of the fit (or the prediction) on new data. Bigger is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97999999999999998"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, svm\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "svc = svm.SVC(C=1, kernel='linear')\n",
    "(svc.fit(X_digits[:-100], y_digits[:-100])\n",
    " .score(X_digits[-100:], y_digits[-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold cross-validation \n",
    "\n",
    "To get a better measure of prediction accuracy (which we can use as a proxy for goodness of fit of the model), we can successively split the data in folds that we use for training and testing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_folds = np.array_split(X_digits, 3)\n",
    "y_folds = np.array_split(y_digits, 3)\n",
    "scores = list()\n",
    "for k in range(3):\n",
    "    # We use 'list to copy, in order to 'pop later on\n",
    "    X_train = list(X_folds)\n",
    "    \n",
    "    # Use the kth array as the test set\n",
    "    X_test = X_train.pop(k)\n",
    "    # Concatenate the two remaining arrays in the training set\n",
    "    X_train = np.concatenate(X_train)\n",
    "    \n",
    "    # Use the same method for the output training and test labels\n",
    "    y_train = list(y_folds)\n",
    "    y_test = y_train.pop(k)\n",
    "    y_train = np.concatenate(y_train)\n",
    "    \n",
    "    # Run the estimator and collect scores\n",
    "    scores.append(svc.fit(X_train, y_train).score(X_test, y_test))\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cross-validation generators \n",
    "The code above to split data in train and test sets is tedious to write. Scikit-learn exposes cross-validation generators to generate list of indices for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [2 3 4 5] | test: [0 1]\n",
      "Train: [0 1 4 5] | test: [2 3]\n",
      "Train: [0 1 2 3] | test: [4 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "k_fold = cross_validation.KFold(n=6, n_folds=3)\n",
    "for train_indices, test_indices in k_fold:\n",
    "    print('Train: %s | test: %s' % (train_indices, test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation can then be implemented easily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = cross_validation.KFold(len(X_digits), n_folds=3)\n",
    "[\n",
    "    svc.fit(X_digits[train], y_digits[train])\n",
    "    .score(X_digits[test], y_digits[test])\n",
    "    for train, test in kfold\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the score method of an estimator, the sklearn exposes a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93489149,  0.95659432,  0.93989983])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation.cross_val_score(svc, X_digits, y_digits, cv=kfold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "On the digits dataset, plot the cross-validation score of a SVC estimator with an linear kernel as a function of parameter C (use a logarithmic grid of points, from 1 to 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import cross_validation, datasets, svm\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "svc = svm.SVC(kernel='linear')\n",
    "C_s = np.logspace(-10, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = list()\n",
    "scores_std = list()\n",
    "for C in C_s:\n",
    "    svc.C = C\n",
    "    this_scores = cross_validation.cross_val_score(svc, X, y, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do the plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "plt.semilogx(C_s, scores)\n",
    "plt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')\n",
    "plt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')\n",
    "locs, labels = plt.yticks()\n",
    "plt.yticks(locs, list(map(lambda x: \"%g\" % x, locs)))\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-search and cross-validated estimators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-search \n",
    "The sklearn provides an object that, given data, computes the score during the fit of an estimator on a parameter grid and chooses the parameters to maximize the cross-validation score. This object takes an estimator during the construction and exposes an estimator API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None,\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
       "       param_grid={'C': array([  1.00000e-06,   3.59381e-06,   1.29155e-05,   4.64159e-05,\n",
       "         1.66810e-04,   5.99484e-04,   2.15443e-03,   7.74264e-03,\n",
       "         2.78256e-02,   1.00000e-01])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, score_func=None, scoring=None,\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "Cs = np.logspace(-6, -1, 10)\n",
    "clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs), n_jobs=1)\n",
    "clf.fit(X_digits[:1000], y_digits[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92500000000000004"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0077426368268112772"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94353826850690092"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction performance on test set is not as good as on train set\n",
    "clf.score(X_digits[1000:], y_digits[1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the GridSearchCV uses a 3-fold cross-validation. However, if it detects that a classifier is passed, rather than a regressor, it uses a stratified 3-fold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested cross-validation\n",
    "Two cross-validation loops are performed in parallel: one by the GridSearchCV estimator to set gamma and the other one by cross_val_score to measure the prediction performance of the estimator. The resulting scores are unbiased estimates of the prediction score on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93853821,  0.96327212,  0.94463087])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation.cross_val_score(clf, X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: You cannot nest objects with parallel computing (n_jobs different than 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validated estimators \n",
    "Cross-validation to set a parameter can be done more efficiently on an algorithm-by-algorithm basis. This is why for certain estimators the sklearn exposes Cross-validation: evaluating estimator performance estimators that set their parameter automatically by cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
       "    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,\n",
       "    precompute='auto', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "lasso = linear_model.LassoCV()\n",
    "diabetes = datasets.load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "lasso.fit(X_diabetes, y_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012291895087486173"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51537990942396628"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.score(X_diabetes, y_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "On the diabetes dataset, find the optimal regularization parameter alpha.\n",
    "Bonus: How much can you trust the selection of alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation, datasets, linear_model\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:150]\n",
    "y = diabetes.target[:150]\n",
    "X_test = diabetes.data[150:]\n",
    "y_test = diabetes.target[150:]\n",
    "\n",
    "lasso = linear_model.Lasso()\n",
    "alphas = np.logspace(-4, -.5, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = list()\n",
    "scores_std = list()\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso.alpha = alpha\n",
    "    this_scores = cross_validation.cross_val_score(lasso, X, y, n_jobs=1)\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.semilogx(alphas, scores)\n",
    "# plot error lines showing +/- std. errors of scores\n",
    "plt.semilogx(alphas, np.array(scores) + np.array(scores_std) / np.sqrt(len(X)), 'b--')\n",
    "plt.semilogx(alphas, np.array(scores) - np.array(scores_std) / np.sqrt(len(X)), 'b--')\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fold 0] alpha: 0.10405, score: 0.53573\n",
      "[fold 1] alpha: 0.05968, score: 0.16278\n",
      "[fold 2] alpha: 0.10405, score: 0.44437\n"
     ]
    }
   ],
   "source": [
    "# Bonus: how much can you trust the selection of alpha?\n",
    "\n",
    "lasso_cv = linear_model.LassoCV(alphas=alphas)\n",
    "k_fold = cross_validation.KFold(len(X), 3)\n",
    "for k, (train, test) in enumerate(k_fold):\n",
    "    lasso_cv.fit(X[train], y[train])\n",
    "    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\"\n",
    "          .format(k, lasso_cv.alpha_, \n",
    "                  lasso_cv.score(X[test], y[test])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer: not much "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
