{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/tutorial/statistical_inference/unsupervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: grouping observations together \n",
    "#### The problem solved in clustering\n",
    "Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a clustering task: split the observations into well-separated group called clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cluster, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(k_means.labels_[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_iris[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application example: vector quantization \n",
    "\n",
    "Clustering in general and KMeans, in particular, can be seen as a way of choosing a small number of exemplars to compress the information. The problem is sometimes known as vector quantization. For instance, this can be used to posterize an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Get face image?\n",
    "try:\n",
    "    face = sp.face(gray=True)\n",
    "except AttributeError:\n",
    "    from scipy import misc\n",
    "    face = misc.face(gray=True)\n",
    "\n",
    "# Create an (n_sample, n_feature) array\n",
    "X = face.reshape((-1, 1))\n",
    "k_means = cluster.KMeans(n_clusters=5, n_init=1)\n",
    "k_means.fit(X)\n",
    "values = k_means.cluster_centers_.squeeze()\n",
    "labels = k_means.labels_\n",
    "face_compressed = np.choose(labels, values)\n",
    "face_compressed.shape = face.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_clusters = 5\n",
    "np.random.seed(0)\n",
    "\n",
    "vmin = face.min()\n",
    "vmax = face.max()\n",
    "\n",
    "# original face\n",
    "plt.figure(1, figsize=(3, 2.2))\n",
    "plt.imshow(face, cmap=plt.cm.gray, vmin=vmin, vmax=256)\n",
    "\n",
    "# compressed face\n",
    "plt.figure(2, figsize=(3, 2.2))\n",
    "plt.imshow(face_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
    "\n",
    "# equal bins face\n",
    "regular_values = np.linspace(0, 256, n_clusters + 1)\n",
    "regular_labels = np.searchsorted(regular_values, face) - 1\n",
    "regular_values = .5 * (regular_values[1:] + regular_values[:-1])  # mean\n",
    "regular_face = np.choose(regular_labels.ravel(), regular_values, mode=\"clip\")\n",
    "regular_face.shape = face.shape\n",
    "plt.figure(3, figsize=(3, 2.2))\n",
    "plt.imshow(regular_face, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
    "\n",
    "# histogram\n",
    "plt.figure(4, figsize=(3, 2.2))\n",
    "plt.clf()\n",
    "plt.axes([.01, .01, .98, .98])\n",
    "plt.hist(X, bins=256, color='.5', edgecolor='.5')\n",
    "plt.yticks(())\n",
    "plt.xticks(regular_values)\n",
    "values = np.sort(values)\n",
    "for center_1, center_2 in zip(values[:-1], values[1:]):\n",
    "    plt.axvline(.5 * (center_1 + center_2), color='b')\n",
    "\n",
    "for center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):\n",
    "    plt.axvline(.5 * (center_1 + center_2), color='b', linestyle='--')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical agglomerative clustering: Ward \n",
    "\n",
    "A Hierarchical clustering method is a type of cluster analysis that aims to build a hierarchy of clusters. In general, the various approaches of this technique are either:\n",
    "\n",
    " * **Agglomerative** \n",
    "     * Bottom-up approach \n",
    "     * Each observation starts in its own cluster, and clusters are iterativelly merged in such a way to minimize a linkage criterion. \n",
    "     * This approach is particularly interesting when the clusters of interest are made of only a few observations. \n",
    "     * When the number of clusters is large, it is much more computationally efficient than k-means.\n",
    "     \n",
    " * **Divisive**\n",
    "     * Top-down approach \n",
    "     * All observations start in one cluster, which is iteratively split as one moves down the hierarchy. \n",
    "     * For estimating large numbers of clusters, this approach is both slow (due to all observations starting as one cluster, which it splits recursively) and statistically ill-posed.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connectivity-constrained clustering\n",
    "With agglomerative clustering, it is possible to specify which samples can be clustered together by giving a connectivity graph. Graphs in the scikit are represented by their adjacency matrix. Often, a sparse matrix is used. This can be useful, for instance, to retrieve connected regions (sometimes also referred to as connected components) when clustering an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.utils.testing import SkipTest\n",
    "try:\n",
    "    from sklearn.utils.fixes import sp_version\n",
    "except ImportError:\n",
    "    pass\n",
    "    \n",
    "# if sp_version < (0, 12):\n",
    "#     raise SkipTest(\"Skipping because SciPy version earlier than 0.12.0 and\"\n",
    "#                    \" thus does not include the scipy.misc.face() image\")\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "try:\n",
    "    face = sp.face(gray=True)\n",
    "except AttributeError:\n",
    "    from scipy import misc\n",
    "    face = misc.face(gray=True)\n",
    "    \n",
    "# face = sp.misc.imresize(face, 0.10) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(face)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature agglomeration \n",
    "We have seen that sparsity could be used to mitigate the curse of dimensionality, i.e an insufficient amount of observations compared to the number of features. Another approach is to merge together similar features: **feature agglomeration**. This approach can be implemented by clustering in the feature direction, in other words clustering the transposed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster, datasets\n",
    "import numpy as np\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "images = digits.images\n",
    "X = np.reshape(images, (len(images), -1))\n",
    "connectivity = grid_to_graph(*images[0].shape)\n",
    "agglo = cluster.FeatureAgglomeration(connectivity=connectivity, n_clusters=32)\n",
    "agglo.fit(X)\n",
    "X_reduced = agglo.transform(X)\n",
    "X_approx = agglo.inverse_transform(X_reduced)\n",
    "images_approx = np.reshape(X_approx, images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *transform* and *inverse_transform* methods \n",
    "\n",
    "Some estimators expose a transform method, for instance to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompositions: from a signal to components and loadings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components and loadings\n",
    "\n",
    "If X is our multivariate data, then the problem that we are trying to solve is to rewrite it on a different observational basis: we want to learn loadings L and a set of components C such that X = L C. Different criteria exist to choose the components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis: PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3.42327080e+00   1.00670638e+00   1.00880933e-31]\n"
     ]
    }
   ],
   "source": [
    "# Create a signal with only 2 useful dimensions\n",
    "x1 = np.random.normal(size=100)\n",
    "x2 = np.random.normal(size=100)\n",
    "x3 = x1 + x2\n",
    "X = np.c_[x1, x2, x3]\n",
    "\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA()\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can see, only the 2 first components are useful\n",
    "pca.n_components = 2\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Component Analysis: ICA\n",
    "Independent component analysis (ICA) selects components so that the distribution of their loadings carries a maximum amount of independent information. It is able to recover **non-Gaussian** independent signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate sample data\n",
    "time = np.linspace(0, 10, 2000)\n",
    "s1 = np.sin(2 * time) # Signal 1: sinusoidal signal\n",
    "s2 = np.sign(np.sin(3 * time)) # Signal 2: square signal\n",
    "S = np.c_[s1, s2]\n",
    "S += 0.2 * np.random.normal(size=S.shape)\n",
    "S /= S.std(axis=0) # Standardize data\n",
    "# Mix data\n",
    "A = np.array([[1, 1], [0.5, 2]]) # Mixing matrix\n",
    "X = np.dot(S, A.T) # Generate observations\n",
    "\n",
    "# Compute ICA\n",
    "ica = decomposition.FastICA()\n",
    "S_ = ica.fit_transform(X) # Get the estimated sources\n",
    "A_ = ica.mixing_.T\n",
    "np.allclose(X, np.dot(S_, A_) + ica.mean_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
