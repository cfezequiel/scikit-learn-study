{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: grouping observations together \n",
    "#### The problem solved in clustering\n",
    "Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a clustering task: split the observations into well-separated group called clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, n_init=10,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cluster, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "k_means = cluster.KMeans(n_clusters=3)\n",
    "k_means.fit(X_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 0 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(k_means.labels_[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_iris[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application example: vector quantization \n",
    "\n",
    "Clustering in general and KMeans, in particular, can be seen as a way of choosing a small number of exemplars to compress the information. The problem is sometimes known as vector quantization. For instance, this can be used to posterize an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "# Get face image?\n",
    "try:\n",
    "    face = sp.face(gray=True)\n",
    "except AttributeError:\n",
    "    from scipy import misc\n",
    "    face = misc.face(gray=True)\n",
    "\n",
    "# Create an (n_sample, n_feature) array\n",
    "X = face.reshape((-1, 1))\n",
    "k_means = cluster.KMeans(n_clusters=5, n_init=1)\n",
    "k_means.fit(X)\n",
    "values = k_means.cluster_centers_.squeeze()\n",
    "labels = k_means.labels_\n",
    "face_compressed = np.choose(labels, values)\n",
    "face_compressed.shape = face.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_clusters = 5\n",
    "np.random.seed(0)\n",
    "\n",
    "vmin = face.min()\n",
    "vmax = face.max()\n",
    "\n",
    "# original face\n",
    "plt.figure(1, figsize=(3, 2.2))\n",
    "plt.imshow(face, cmap=plt.cm.gray, vmin=vmin, vmax=256)\n",
    "\n",
    "# compressed face\n",
    "plt.figure(2, figsize=(3, 2.2))\n",
    "plt.imshow(face_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
    "\n",
    "# equal bins face\n",
    "regular_values = np.linspace(0, 256, n_clusters + 1)\n",
    "regular_labels = np.searchsorted(regular_values, face) - 1\n",
    "regular_values = .5 * (regular_values[1:] + regular_values[:-1])  # mean\n",
    "regular_face = np.choose(regular_labels.ravel(), regular_values, mode=\"clip\")\n",
    "regular_face.shape = face.shape\n",
    "plt.figure(3, figsize=(3, 2.2))\n",
    "plt.imshow(regular_face, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
    "\n",
    "# histogram\n",
    "plt.figure(4, figsize=(3, 2.2))\n",
    "plt.clf()\n",
    "plt.axes([.01, .01, .98, .98])\n",
    "plt.hist(X, bins=256, color='.5', edgecolor='.5')\n",
    "plt.yticks(())\n",
    "plt.xticks(regular_values)\n",
    "values = np.sort(values)\n",
    "for center_1, center_2 in zip(values[:-1], values[1:]):\n",
    "    plt.axvline(.5 * (center_1 + center_2), color='b')\n",
    "\n",
    "for center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):\n",
    "    plt.axvline(.5 * (center_1 + center_2), color='b', linestyle='--')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical agglomerative clustering: Ward \n",
    "\n",
    "A Hierarchical clustering method is a type of cluster analysis that aims to build a hierarchy of clusters. In general, the various approaches of this technique are either:\n",
    "\n",
    " * **Agglomerative** \n",
    "     * Bottom-up approach \n",
    "     * Each observation starts in its own cluster, and clusters are iterativelly merged in such a way to minimize a linkage criterion. \n",
    "     * This approach is particularly interesting when the clusters of interest are made of only a few observations. \n",
    "     * When the number of clusters is large, it is much more computationally efficient than k-means.\n",
    "     \n",
    " * **Divisive**\n",
    "     * Top-down approach \n",
    "     * All observations start in one cluster, which is iteratively split as one moves down the hierarchy. \n",
    "     * For estimating large numbers of clusters, this approach is both slow (due to all observations starting as one cluster, which it splits recursively) and statistically ill-posed.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connectivity-constrained clustering\n",
    "With agglomerative clustering, it is possible to specify which samples can be clustered together by giving a connectivity graph. Graphs in the scikit are represented by their adjacency matrix. Often, a sparse matrix is used. This can be useful, for instance, to retrieve connected regions (sometimes also referred to as connected components) when clustering an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
